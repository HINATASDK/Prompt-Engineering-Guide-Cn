# 面向任务上下文学习

基于大型语言模型（LLM）的指令学习开启了零样本任务新范式。然而，指令学习主要作为一个微调问题被接纳，包括指令微调和基于人类反馈的强化学习，在 LLM 上进行多任务微调，按照指令提示完成各种任务。在本文中，我们发现一个惊人的结果，即将上下文学习应用于指令学习，称为上下文指令学习（ICIL），显著提高了预训练模型和指令微调模型的零样本任务泛化性能。ICIL 的核心优势之一是它使用单个固定提示来评估所有任务，这是跨任务演示的连接。特别是，我们证明最强大的指令微调基线（text-davinci-003）也受益于 ICIL，提高了 9.3％，表明 ICIL 的效果是指令微调的补充。

## 介绍

大型语言模型（LLM）证明了在推断过程中通过小样本适配目标任务的能力，也称为上下文指导学习。随着模型大小的扩大，LLM 表现出越来越多的新能力。其中一项新兴能力是通过遵循指令来推广到看不见的任务。已经提出了指令学习方法来改善这种能力，包括指令调整或 RLHF（来自人类反馈的强化学习）。然而，以前的工作主要集中在基于微调的指令学习方法，其中模型在多个任务上进行多任务微调，需要多个反向传播过程。

在这项工作中，我们展示了面向任务上下文学习（In-Context Instruction Learning，ICIL）的学习方法可以促进通用预训练模型和指令跟踪细调模型的性能提升。ICIL 通过一个包含多个跨任务演示的提示来进行学习，其中每个演示是一个任务指令、输入和输出实例的串联形式。由于我们确保用于演示的任务与评估集严格隔离，而且我们将它们作为唯一的固定提示在所有评估任务中使用，因此 ICIL 是一种零样本学习方法. 我们采用了一个简单的启发式抽样方法来构建适用于不同类型下游任务和模型规模的固定演示集。通过为所有任务预先添加相同的固定演示集，我们可以轻松测试和重现新目标任务或模型的基准零样本性能，而不需要依赖外部工具。

我们首先观察到，ICIL 显著增强了未经指令微调的各种预训练 LLM 的零样本任务泛化性能。值得注意的是，即使是较小的 LLM 加入了 ICIL，其性能也胜过未加入 ICIL 的规模更大的语言模型，例如 6B 大小的 ICIL GPT-J 胜过 30 倍大的 175B 大小的标准零样本 GPT-3 Davinci。其次，我们展示了将 ICIL 应用于指令微调 LLM 的情况下，特别是在超过 100B 的模型上，可以提高 LLM 的零样本指令跟随能力。这表明 ICIL 的效果与指令微调的效果相辅相成。

我们的分析表明，ICIL 的有效性来自于选择包含明确答案选项的分类任务。即使是对于生成目标任务，这一点仍然成立，这与以往的研究相反，以往的研究显示在少样本任务上下文中，关键在于检索与目标任务相似的演示。更令人意外的是，我们观察到，即使将每个演示的输入实例分布替换为随机语句，也不会对性能造成显着影响。基于此分析，我们假设 LLM 在进行推理时学习了包含在指令中的答案选项和每个演示的输出之间的对应关系，而不是依赖于指令、输入和输出之间的复杂对应关系。通过这一假设，我们建议 ICIL 的作用是帮助 LLM 关注目标指令，以找到目标任务答案分布的线索。

## 面向任务上下文学习

面向任务上下文学习（In-Context Instruction Learning，ICIL）的提示由跨任务演示组合而成，每个演示由指令、输入和输出实例串联而成。在本节中，我们将解释如何构建一个固定的演示集来以零样本的方式评估 ICIL 的各种任务，并提到在 LLM 的推理过程中应用 ICIL 的优点。

### 演示集构建

从包含 N 个任务的基准测试集中，每个任务实例由指令、输入和输出实例组成，我们随机选择 K 个任务作为 ICIL 的演示任务。我们应用了一些简单的启发式方法来筛选任务集，随机在筛选后的任务集中抽取一个实例，最后抽取 K 个不同任务对应的实例作为演示集。

这些启发式方法如下：

- 任务类型：我们只从包含答案选项的分类任务中进行抽样。我们假设在指令中包含答案选项可以帮助 LLM 在推理过程中遵循指令。
- 答案选项重叠：我们确保演示任务之间不重复包含答案选项。我们预计，答案选项的重叠会导致 LLM 复制演示的标签，类似于少样本上下文中的学习，这是零样本学习不应该出现的行为，因为答案分布会随着目标任务而变化。
- 演示长度：我们通过一个最大限制来限制演示的长度（指令、输入和输出实例的串联），确保输入实例不超过最大序列长度 256 个令牌。我们只从符合此标准的实例中进行抽样。
- 演示排序：我们按照升序对每个任务中的答案选项数量对演示进行排序。对于具有相同答案选项数量的演示，我们按长度进行升序排序。

我们在第 4 节中提供了这些启发式方法的详细分析和消融实验（ablation experiment)。

### 推理时的面向任务上下文学习

演示集抽样后，我们构建了一组固定的演示，并将目标任务的指令和输入实例的串联附加到由演示组成的固定提示中。

ICIL 具有以下优点，可以提高 LLM 的指令跟随能力和零样本任务泛化性能：

- ICIL 使用一个单一的固定提示来适应不同的模型和任务。因此，不需要外部工具来搜索和检索每个任务的最佳演示集，ICIL 易于复制和测量作为新模型或数据集的零样本基线。
- 我们展示了 ICIL 显著提高了多种通用预训练 LLM 的零样本任务泛化性能。这表明我们可以使 LLM 成为更好的指令跟随者，而不需要反向传播。
- ICIL 还提高了指令微调模型的性能，尤其是对于参数超过 100B 的 LLM 模型。这表明即使在完成指令微调或强化语言模型微调训练后，ICIL 也能为零样本泛化提供帮助，暗示着 ICIL 的广泛适用性。
- 我们证明了模型生成的演示集对于 ICIL 也是有效的（第 4.2 节）。这表明即使没有从基准测试集中抽取演示集，只要应用了启发式方法，ICIL 仍然有效

## 实验

### 实验设置

我们从 SUPER-NATURALINSTRUCTIONS（SUPERNI）基准测试集的英文训练任务中构建了 ICIL 的演示集，该测试集包含总共 756 个任务。为了评估 ICIL 的有效性，我们使用 SUPERNI 中保留的任务作为测试集，包括 12 个不同类别的 119 个任务，包括自由形式生成、单词关系推理和分类任务。我们选择 SUPERNI 作为评估基准测试集，因为它提供了一组复杂程度不同的任务。每个任务有 100 个实例，我们排除了超过最大序列长度的实例，共计 11,802 个实例。对于每个任务，我们使用不同的评估指标，例如对于分类或单词预测任务，我们使用 Exact Match 指标；对于自由形式生成任务，我们使用 ROUGE-L 指标，这与 Wang 等人（2022c）使用的指标相同。

我们评估了四种具有不同模型大小的 LLM 模型类型：1）GPT-3，2）OPT，3）GPT-NeoX，4）GPT-J 。对于 GPT-3，我们不仅评估预训练的 LLM，还评估经过强化学习微调的 LLM，使其能够遵循指令并与人类偏好保持一致（Ouyang 等人，2022）。我们评估了大小为 6.7B 和 175B 的 GPT-3 模型的性能。对于 OPT，我们评估了具有 6.7B、13B 和 30B 参数的模型，而对于 GPT-NeoX 和 GPT-J，我们分别评估具有 20B 和 6B 参数的模型。

### 结果如下

各种预训练的 LLM 收益于 ICIL，In-Context Instruction Learning（ICIL）持续改善了预训练 LLM 在所有模型规模上的性能，使 OPT-13B 的性能提高超过 50％。这种简单的零样本上下文学习 能够胜过参数更大的 LLM 模型。具体来说，带有 ICIL 的 6B 大小的 GPT-J 模型超过 30 倍于 175B 大小的 GPT-3 模型的性能。这表明 ICIL 提高了预训练 LLM 遵循指令的能力，而不需要进行微调或反向传播。此外，我们发现，在推理过程中，ICIL 的增益与通过将 ICIL 应用于未进行指令调整和经过指令调整的 GPT-3 模型的标准零样本设置之间的性能比较相当。

ICIL 的增益是对基于微调的指令学习的补充。我们观察到 ICIL 可以提高通过指令调整或 RLHF 微调的 LLM 的性能，尤其是对于大于 100B 参数的模型。这意味着，基于微调的指令学习有时可能不足以支持更大的模型，而 上下文指令学习 可以正交地提高指令的遵循能力。特别地，我们观察到 text-davinci-002（175B）的性能显著提高，优于经过标准零样本学习进行 RLHF 微调的 text-davinci-003 模型。此外，我们通过 9.3％的 ICIL 增益证明了最强大的模型（text-davinci-003）也受益于 ICIL，实现了最佳性能。我们将留待未来的工作对更多种类的指令微调模型进行详细分析。

与 ICIL 相关的上下文指令学习并不会对性能造成太大的损害。我们观察到，破坏每个 ICIL 演示中输入实例的分布不会对性能造成太大的损害，这与 Min 在少量数据情况下观察到的情况类似。与 Min 等人（2022b）对输入输出对应关系的扰动不同，我们扰动输入分布本身，这是会有更多扰动的一种情况。根据 Min 等人（2022b）的方法，我们使用 CC-News（Hamborg 等人，2017）作为外部语料库，用具有类似长度的随机句子替换实际输入实例。破坏每个演示的输入实例分布不会对大多数模型规模产生太大的影响。这符合先前研究结果：LLM 模型并不能充分利用它们所接收到的所有信息。有趣的是，在少样本上下文学习中，扰动输入分布本身会导致性能显著下降，但我们证明在 ICIL 中不仅输入输出对应关系不重要，而且输入实例的分布对 ICIL 的影响也很小。

## 分析

本节中，我们将分析使 ICIL 有效的因素，并提供附加实验。由于推理成本，我们只在预训练的 GPT-3 175B 检查点（davinci）上进行评估，并在每个任务类别中仅评估单个任务，因此总共有 12 个任务。

### 削减实验研究

我们进一步分析了每个演示组成部分对 ICIL 的有效性的影响，通过破坏每个组成部分（指令，输入和输出实例）的分布来展开实验。对于指令破坏，我们从外部语料库中用随机序列替换了真实序列，这类似于我们在 3.2 节中讨论的对输入分布的破坏方法。对于输出破坏，我们遵循了 Min 等人（2022b）的方法，用随机英语单词替换了真实标签。破坏每个演示的指令或输出实例的分布会显著损害性能。尤其是，与标准零样本学习相比，破坏指令分布的改进效果很小（31.18 与 29.67 之间）。这表明，与输入实例不同，指令和输出实例的分布显著影响了 ICIL 的性能。

在构建演示集时，使用分类任务是很重要的。我们通过改变演示集中分类任务的比例来分析仅使用分类任务构建演示集的启发式方法在 ICIL 中的效果。当分类任务的比例增加时，零样本任务的平均泛化性能也随之增加。有趣的是，我们观察到将演示集构建为分类任务也有助于生成（非分类）目标任务。这一发现与少样本上下文学习设置形成了鲜明对比，后者中，检索与目标查询相似的演示可以提高少样本性能（Rubin 等人，2021; Liu 等人，2021）。

增加演示数量可以提高性能。我们研究了演示数量对 ICIL 的影响。随着演示数量的增加，平均性能也得到改善，类似于少样本上下文学习。值得注意的是，即使只有 2 个例子，ICIL 的零样本指令跟随能力显著提高，这意味着仅使用少量的零样本演示可以改善 LLM 的性能。

按答案选择数量对演示进行排序可以降低方差。为了检验不同演示集排序方式的影响，我们将基于答案选择数量的 ICIL 排序与随机排序进行比较。尽管平均性能在两种设置之间没有显着差异，但我们观察到基于答案选择数量的排序启发式有助于降低方差，并提高最坏情况的准确率。

演示之间答案选择的重叠会损害性能。我们分析了演示集构建时使用的启发式方法之一——演示之间的答案选择重叠对性能的影响，将使用于 ICIL 的演示集与所有演示都具有相同答案选择的演示集进行了比较。我们观察到，具有答案选择重叠的演示集的平均表现比没有重叠的演示集差，特别是对于生成任务。我们发现，演示集中答案选择重叠会导致模型为长文本生成生成短序列，或者通过复制演示集标签之一来预测输出，导致泛化能力差。

### 附加实验

CIL 同样适用于机器生成的演示集。我们探讨了在使用机器生成的演示任务而非从 SUPERNI 基准的训练任务中进行采样的情况下，ICIL 是否同样有效。我们使用 ChatGPT（OpenAI，2022）进行演示生成，通过指定用于构建 ICIL 演示集的启发式方法来生成演示。ICIL 同样适用于机器生成的演示，显示出与来自 SUPERNI 的演示相当的性能，远优于标准零样本设置。这项发现表明，即使没有从包含各种指导的基准中进行采样，ICIL 仍然有效，这表明性能提升不是通过采样构建演示集，而是通过 ICIL 的启发式方法和格式实现的。

ICIL 的性能与自适应上下文学习方法相当。我们将采样固定演示集的 ICIL 与自适应零样本上下文学习进行了比较（Lyu 等人，2022），后者根据目标任务或实例的相似度变化而检索演示。与 ICIL 类似，我们从 SUPERNI 基准的训练任务中检索包含指令、输入和输出实例的演示。我们使用 SimCSE（Gao 等人，2021）计算序列嵌入，并使用余弦相似度检索每个目标任务或实例的前 K 个相似实例。我们将自适应上下文学习设置分为任务和实例两种，其中前者仅根据指令相似性检索，后者根据指令和输入实例拼接的相似性进行检索。对于使用固定演示集的 ICIL，任务自适应和实例自适应的表现都与其相当。因此，这表明尽管与自适应上下文学习方法相当，ICIL 的固定演示集具有更好的重复性，并且不需要用于相似度搜索的外部嵌入模型。

ICIL 仍有改进的空间。我们将 ICIL 的表现与少样本上下文学习进行了比较，后者是任务自适应的上限。我们使用 8 个样本的上下文学习进行比较，控制演示数量的因素。虽然 ICIL 显著优于标准零样本设置的零样本任务泛化性能，但我们观察到 ICIL 与少样本上下文学习之间仍存在很大的差距.

## 讨论

从之前的章节中我们可以看出，ICIL 显著提高了预训练和指令微调 LLM 的性能。同时，我们还证明了改变输入分布并不会对性能造成太大损害，分析演示集的构建关键在于来自分类任务的演示。在本节中，我们基于之前章节的发现，提出了 ICIL 的作用。

为什么从分类任务中构建演示集很重要？使用分类任务构建演示集对 ICIL 很重要。那么，分类任务与生成（非分类）任务之间有什么区别？因为我们构建演示集的方法之一是仅考虑包含指令中答案选择的分类任务，这些演示具有更明确的答案分布线索。我们猜想，在推断过程中，LLM 可以通过演示学习从指令中的答案选择到标签的对应关系。特别地，因为标签单词出现在分类任务的指令中，LLM 很容易利用这种关系。我们观察到，仅删除包含指令中答案选择的句子会导致 ICIL 性能降低（44.27→42.89），这支持我们的假设。

ICIL 中演示输入分布并不太重要，而指令和输出分布则非常重要。这一观察结果支持了上述假设，即 LLM 在 ICIL 过程中学习了应答指令中答案选择和演示标签之间的对应关系。相比于依赖指令、输入和输出之间的复杂对应关系，LLMs 更倾向于关注指令（包括答案选择）和标签之间的简单对应关系，例如字符串匹配。之前的研究也证明了类似的发现，即 LLM 很容易适应任务，类似于捷径学习.

ICIL 的作用是什么？如果 LLM 在 ICIL 过程中学习了应答指令中答案选择和演示标签之间的对应关系，那么这如何有助于零样本任务的泛化？我们假设在 ICIL 过程中，演示给出了一个信号，让 LLM 将注意力集中在指令上，以找到答案分布的线索，从而更好地遵循指令。我们认为这个假设可以解释为什么从分类任务构建演示集也会提高生成目标任务的性能。虽然指令微调也有助于集中注意力在指令上的信号，但我们假设 ICIL 直接加强了推理过程中指令和演示标签之间的对应关系。

## 相关工作

最近的研究表明，包括指令微调或强化学习的指令学习可以增强 LLMs 遵循指令或与人类偏好保持一致的能力。然而，LLM 的指令跟随能力是通过指令微调新获得的还是在预训练期间已经获得的仍未被深入探索。LLM 自身生成的包含嘈杂实例的下游任务实际上可以成为指令微调的良好训练实例，这意味着 LLM 已经在一定程度上意识到指令的存在。我们通过将语境学习应用于指令学习来扩展这个假设，这并不需要任何反向传播，使用预训练模型的检查点而无需进行任何梯度更新，从而证实 LLM 已经具备跟随指令的能力。

在语境学习中，大型语言模型通过自回归方式预测下一个词汇，并且只依靠少量示例来适应目标任务，而无需进行梯度更新。Brown 等人（2020 年）；Chowdhery 等人（2022 年）已经证明了语境学习的存在。然而，语境学习的内在机制还未深入研究。Akyürek 等人（2022 年）；von Oswald 等人（2022 年）；Garg 等人（2022 年）；Dai 等人（2022）认为，语言模型在语境学习期间执行隐式元微调。然而，Min 等人（2022b 年）发现，为演示分配随机标签并不会对语境学习产生太大影响。基于这个发现，Lyu 等人（2022 年）提出了一种零样本语境学习方法，从外部语料库中检索相关句子，并分配随机标签以构建分类目标任务的演示。与 Lyu 等人（2022）不同，ICIL 利用指令来促进任务适应，在固定的演示集合下评估所有任务，并适用于生成目标任务。

## 限定条件

虽然基于上下文的指导学习法（In-Context Instruction Learning，ICIL）能够展现出卓越的零样本任务泛化性能，但是相比于其他模型，因为输入序列的数量增加，它在推理过程中会遭受计算量大的问题。同时，在少样本学习方面，该模型仍然存在着相当大的性能差距，详见图 6b。需要注意的是，这只是一个正在进行中版本，我们计划在未来对 ICIL 在各个场景和基准测试中进行广泛的分析和评估。

## 结论

本文研究表明，在推理过程中通过固定一组示范学习跟随指导的能力，即基于上下文的指导学习法（In-Context Instruction Learning，ICIL），能够显著提高预训练和指导微调的语言模型（LLM）的零样本任务泛化性能。通过详细分析，我们假设，ICIL 的效果源于学习指令中答案选项与示范品中标签相一致的关系，从而使 LLM 更好地关注指令内容。总之，如果愿意在推理速度上做出一定的牺牲以获得更高的准确性，我们推荐认真考虑 ICIL 以最大程度地提高零样本任务泛化性能。
